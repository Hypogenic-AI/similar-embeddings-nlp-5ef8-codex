\section{Abstract}
\label{sec:abstract}
Multilingual encoders are widely used for cross-lingual retrieval and transfer, yet it remains unclear whether cosine similarity in these spaces meaningfully distinguishes shared senses when words are polysemous. We examine this question with \xlmr by comparing (i) static similarity for translation pairs from \muse dictionaries and (ii) contextual similarity for sense-labeled cross-lingual pairs from \mclwic. Our analysis shows that translation pairs are much more similar than mismatched pairs (mean 0.513 vs. 0.375; Cohen's $d\approx1.03$), confirming strong lexical alignment. For contextual embeddings, same-sense pairs are only slightly more similar than different-sense pairs (0.9847 vs. 0.9822; $d\approx0.46$), despite highly significant tests. A simple threshold trained on development data yields 0.53 accuracy on cross-lingual \mclwic test sets, only marginally above chance. These results suggest that multilingual contextual embeddings preserve shared meaning at a coarse level but that raw cosine similarity is too blunt for robust sense discrimination without additional modeling. Our findings provide practical guidance for cross-lingual semantic search and lexicon induction: embedding similarity is reliable for translation-level alignment but only weakly sensitive to sense mismatches.
